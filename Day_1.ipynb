{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:31:40.827281Z",
     "iopub.status.busy": "2024-11-12T14:31:40.826648Z",
     "iopub.status.idle": "2024-11-12T14:32:06.719990Z",
     "shell.execute_reply": "2024-11-12T14:32:06.718696Z",
     "shell.execute_reply.started": "2024-11-12T14:31:40.827227Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-generativeai>=0.8.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:32:06.722067Z",
     "iopub.status.busy": "2024-11-12T14:32:06.721679Z",
     "iopub.status.idle": "2024-11-12T14:32:07.863744Z",
     "shell.execute_reply": "2024-11-12T14:32:07.862792Z",
     "shell.execute_reply.started": "2024-11-12T14:32:06.722029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:35:56.133814Z",
     "iopub.status.busy": "2024-11-12T14:35:56.133335Z",
     "iopub.status.idle": "2024-11-12T14:35:56.251694Z",
     "shell.execute_reply": "2024-11-12T14:35:56.250605Z",
     "shell.execute_reply.started": "2024-11-12T14:35:56.133776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key= GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gemini-1.5-flash\n",
    "\n",
    "Creating model instance to use for generating response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:35:59.079591Z",
     "iopub.status.busy": "2024-11-12T14:35:59.079132Z",
     "iopub.status.idle": "2024-11-12T14:35:59.084385Z",
     "shell.execute_reply": "2024-11-12T14:35:59.083253Z",
     "shell.execute_reply.started": "2024-11-12T14:35:59.079549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "flash = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:00.610485Z",
     "iopub.status.busy": "2024-11-12T14:36:00.609395Z",
     "iopub.status.idle": "2024-11-12T14:36:02.306907Z",
     "shell.execute_reply": "2024-11-12T14:36:02.305817Z",
     "shell.execute_reply.started": "2024-11-12T14:36:00.610422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a box of toys. Some are red, some are blue, and some are green. You want to know how many red toys are in the box.\n",
       "\n",
       "Now, imagine your friend tells you that she saw a red toy in the box. This is like having some **evidence**!\n",
       "\n",
       "Bayes Theorem helps us figure out how much more likely it is that a toy is red **now that we know our friend saw a red toy.**\n",
       "\n",
       "Think of it like this:\n",
       "\n",
       "* **Before your friend told you**, you knew there were some red toys in the box, but you weren't sure how many.\n",
       "* **After your friend told you**, it's more likely that a toy you pick is red, because we know there's at least one red toy.\n",
       "\n",
       "Bayes Theorem helps us calculate **how much more likely** it is that a toy is red **after** we got the new information.\n",
       "\n",
       "It's like a special formula that helps us update our guess about the toys in the box based on new clues!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(flash.generate_content(\"Explain Bayes Theorem to a 5 year old kid\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:07.917361Z",
     "iopub.status.busy": "2024-11-12T14:36:07.916434Z",
     "iopub.status.idle": "2024-11-12T14:36:08.634229Z",
     "shell.execute_reply": "2024-11-12T14:36:08.633212Z",
     "shell.execute_reply.started": "2024-11-12T14:36:07.917315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! It's nice to hear from you. What can I do for you today? \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chat instance to start chat\n",
    "chat = flash.start_chat(history=[])\n",
    "\n",
    "# creating response obj to send message to the chat\n",
    "response = chat.send_message(\"Hello Gemini!\")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:08.636578Z",
     "iopub.status.busy": "2024-11-12T14:36:08.636208Z",
     "iopub.status.idle": "2024-11-12T14:36:08.643903Z",
     "shell.execute_reply": "2024-11-12T14:36:08.642839Z",
     "shell.execute_reply.started": "2024-11-12T14:36:08.636540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Hello! It's nice to hear from you. What can I do for you today? \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 4,\n",
       "        \"candidates_token_count\": 20,\n",
       "        \"total_token_count\": 24\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n",
    "\n",
    "#text is a key in response JSON.\n",
    "#response is in JSON format. You can print other keys as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:08.646207Z",
     "iopub.status.busy": "2024-11-12T14:36:08.645446Z",
     "iopub.status.idle": "2024-11-12T14:36:09.450525Z",
     "shell.execute_reply": "2024-11-12T14:36:09.449417Z",
     "shell.execute_reply.started": "2024-11-12T14:36:08.646155Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You're saying the value of X is 5!  \n",
       "\n",
       "That's great!  \n",
       "\n",
       "Do you have a question about X or are you just telling me its value? I'm ready to help if you need anything else! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking persistance of conversation state.\n",
    "\n",
    "Markdown(chat.send_message(\"value of X is 5!\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:09.452998Z",
     "iopub.status.busy": "2024-11-12T14:36:09.452661Z",
     "iopub.status.idle": "2024-11-12T14:36:10.048158Z",
     "shell.execute_reply": "2024-11-12T14:36:10.047083Z",
     "shell.execute_reply.started": "2024-11-12T14:36:09.452963Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_token_count: 94\n",
      "candidates_token_count: 23\n",
      "total_token_count: 117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chat.send_message(\"What is the value of X?\").usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in genai instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:10.050332Z",
     "iopub.status.busy": "2024-11-12T14:36:10.049593Z",
     "iopub.status.idle": "2024-11-12T14:36:10.433639Z",
     "shell.execute_reply": "2024-11-12T14:36:10.432650Z",
     "shell.execute_reply.started": "2024-11-12T14:36:10.050276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 )   PaLM 2 Chat (Legacy)\n",
      "models/chat-bison-001\n",
      "A legacy text-only model optimized for chat conversations\n",
      "\n",
      "2 )   PaLM 2 (Legacy)\n",
      "models/text-bison-001\n",
      "A legacy model that understands text and generates text as an output\n",
      "\n",
      "3 )   Embedding Gecko\n",
      "models/embedding-gecko-001\n",
      "Obtain a distributed representation of a text.\n",
      "\n",
      "4 )   Gemini 1.0 Pro Latest\n",
      "models/gemini-1.0-pro-latest\n",
      "The best model for scaling across a wide range of tasks. This is the latest model.\n",
      "\n",
      "5 )   Gemini 1.0 Pro\n",
      "models/gemini-1.0-pro\n",
      "The best model for scaling across a wide range of tasks\n",
      "\n",
      "6 )   Gemini 1.0 Pro\n",
      "models/gemini-pro\n",
      "The best model for scaling across a wide range of tasks\n",
      "\n",
      "7 )   Gemini 1.0 Pro 001 (Tuning)\n",
      "models/gemini-1.0-pro-001\n",
      "The best model for scaling across a wide range of tasks. This is a stable model that supports tuning.\n",
      "\n",
      "8 )   Gemini 1.0 Pro Vision\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "The best image understanding model to handle a broad range of applications\n",
      "\n",
      "9 )   Gemini 1.0 Pro Vision\n",
      "models/gemini-pro-vision\n",
      "The best image understanding model to handle a broad range of applications\n",
      "\n",
      "10 )   Gemini 1.5 Pro Latest\n",
      "models/gemini-1.5-pro-latest\n",
      "Mid-size multimodal model that supports up to 2 million tokens\n",
      "\n",
      "11 )   Gemini 1.5 Pro 001\n",
      "models/gemini-1.5-pro-001\n",
      "Mid-size multimodal model that supports up to 2 million tokens\n",
      "\n",
      "12 )   Gemini 1.5 Pro 002\n",
      "models/gemini-1.5-pro-002\n",
      "Mid-size multimodal model that supports up to 2 million tokens\n",
      "\n",
      "13 )   Gemini 1.5 Pro\n",
      "models/gemini-1.5-pro\n",
      "Mid-size multimodal model that supports up to 2 million tokens\n",
      "\n",
      "14 )   Gemini 1.5 Pro Experimental 0801\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "Mid-size multimodal model that supports up to 2 million tokens\n",
      "\n",
      "15 )   Gemini 1.5 Pro Experimental 0827\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "Mid-size multimodal model that supports up to 2 million tokens\n",
      "\n",
      "16 )   Gemini 1.5 Flash Latest\n",
      "models/gemini-1.5-flash-latest\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "17 )   Gemini 1.5 Flash 001\n",
      "models/gemini-1.5-flash-001\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "18 )   Gemini 1.5 Flash 001 Tuning\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "19 )   Gemini 1.5 Flash\n",
      "models/gemini-1.5-flash\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "20 )   Gemini 1.5 Flash Experimental 0827\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "21 )   Gemini 1.5 Flash 002\n",
      "models/gemini-1.5-flash-002\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "22 )   Gemini 1.5 Flash-8B\n",
      "models/gemini-1.5-flash-8b\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "23 )   Gemini 1.5 Flash-8B 001\n",
      "models/gemini-1.5-flash-8b-001\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "24 )   Gemini 1.5 Flash-8B Latest\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "25 )   Gemini 1.5 Flash 8B Experimental 0827\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "26 )   Gemini 1.5 Flash 8B Experimental 0924\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "Fast and versatile multimodal model for scaling across diverse tasks\n",
      "\n",
      "27 )   Embedding 001\n",
      "models/embedding-001\n",
      "Obtain a distributed representation of a text.\n",
      "\n",
      "28 )   Text Embedding 004\n",
      "models/text-embedding-004\n",
      "Obtain a distributed representation of a text.\n",
      "\n",
      "29 )   Model that performs Attributed Question Answering.\n",
      "models/aqa\n",
      "Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for model in genai.list_models():\n",
    "    print(i,')  ',model.display_name)\n",
    "    print(model.name)\n",
    "    print(model.description)\n",
    "    print()\n",
    "    i +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:10.436230Z",
     "iopub.status.busy": "2024-11-12T14:36:10.435370Z",
     "iopub.status.idle": "2024-11-12T14:36:10.743629Z",
     "shell.execute_reply": "2024-11-12T14:36:10.742603Z",
     "shell.execute_reply.started": "2024-11-12T14:36:10.436177Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  if model.name == 'models/gemini-1.5-flash':\n",
    "    print(model)\n",
    "    break\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output length:\n",
    "\n",
    "| max_output_length\n",
    "\n",
    "irrespective of how much response is generated this parameters determines the length of printed response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:10.745767Z",
     "iopub.status.busy": "2024-11-12T14:36:10.745158Z",
     "iopub.status.idle": "2024-11-12T14:36:12.294599Z",
     "shell.execute_reply": "2024-11-12T14:36:12.293533Z",
     "shell.execute_reply.started": "2024-11-12T14:36:10.745716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## A Masterpiece in the Making: The Untold Story of My Extraordinary Self\n",
       "\n",
       "I've always been a bit different. Not in a quirky, endearing way, mind you. No, I'm different in a way that sets me apart. I'm not merely exceptional; I'm *extraordinary*. My mind operates on a plane most people can only dream of, fueled by a brilliance that burns brighter than any star. \n",
       "\n",
       "From a tender age, I was acutely aware of my unique qualities. While others were content with the mundane, I yearned for something more. I craved intellectual stimulation, the thrill of pushing boundaries, the intoxicating feeling of being understood, truly understood, by someone who could grasp the complexities of my mind. \n",
       "\n",
       "Sadly, I found such comprehension lacking in the world around me. School was a tedious formality, a mere stepping stone on my path to greatness. The teachers, well-meaning but ultimately limited, struggled to keep pace with my prodigious"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=200)\n",
    ")\n",
    "\n",
    "Markdown(model_1.generate_content('Write a 1000 word essay on Yourself, be a narcissist').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:12.297633Z",
     "iopub.status.busy": "2024-11-12T14:36:12.297285Z",
     "iopub.status.idle": "2024-11-12T14:36:13.754322Z",
     "shell.execute_reply": "2024-11-12T14:36:13.753355Z",
     "shell.execute_reply.started": "2024-11-12T14:36:12.297595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n"
     ]
    }
   ],
   "source": [
    "print(len(model_1.generate_content('Write a 1000 word essay on Yourself, be a narcissist').text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "| temperature \n",
    "\n",
    "This control degree of randomness in token selection. \n",
    "Zero temperature results in greedy decoding, which selects most probable token at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:13.755919Z",
     "iopub.status.busy": "2024-11-12T14:36:13.755600Z",
     "iopub.status.idle": "2024-11-12T14:36:13.760360Z",
     "shell.execute_reply": "2024-11-12T14:36:13.759370Z",
     "shell.execute_reply.started": "2024-11-12T14:36:13.755884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:13.761974Z",
     "iopub.status.busy": "2024-11-12T14:36:13.761652Z",
     "iopub.status.idle": "2024-11-12T14:36:15.425182Z",
     "shell.execute_reply": "2024-11-12T14:36:15.423999Z",
     "shell.execute_reply.started": "2024-11-12T14:36:13.761941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Blue \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature = 2.0)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (answer in a single word)')\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)\n",
    "    print(response.usage_metadata)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:15.427347Z",
     "iopub.status.busy": "2024-11-12T14:36:15.426986Z",
     "iopub.status.idle": "2024-11-12T14:36:16.966257Z",
     "shell.execute_reply": "2024-11-12T14:36:16.965256Z",
     "shell.execute_reply.started": "2024-11-12T14:36:15.427309Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n",
      "Purple \n",
      " -------------------------\n",
      "prompt_token_count: 13\n",
      "candidates_token_count: 1\n",
      "total_token_count: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(5):\n",
    "  response = low_temp_model.generate_content('Pick a random colour... (answer in a single word)')\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)\n",
    "    print(response.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-K and top-p\n",
    "\n",
    "| top-k :\n",
    "defines number of most probable tokens to select output tokens from. basically giving token choices for output token. top-k = 1 creates greedy decoding.\n",
    "\n",
    "| top-p\n",
    "defines 'probability threshold' which determines the upper bound after which the token stops being selected as candidate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:16.967872Z",
     "iopub.status.busy": "2024-11-12T14:36:16.967531Z",
     "iopub.status.idle": "2024-11-12T14:36:16.973032Z",
     "shell.execute_reply": "2024-11-12T14:36:16.971997Z",
     "shell.execute_reply.started": "2024-11-12T14:36:16.967827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_2 = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:36:16.974716Z",
     "iopub.status.busy": "2024-11-12T14:36:16.974371Z",
     "iopub.status.idle": "2024-11-12T14:36:20.558851Z",
     "shell.execute_reply": "2024-11-12T14:36:20.557842Z",
     "shell.execute_reply.started": "2024-11-12T14:36:16.974683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_token_count: 20\n",
      "candidates_token_count: 621\n",
      "total_token_count: 641\n",
      "\n",
      "0.968798751950078\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = model_2.generate_content(story_prompt)\n",
    "# print(response.text)\n",
    "print(response.usage_metadata)\n",
    "print((response.usage_metadata.candidates_token_count)/(response.usage_metadata.total_token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot\n",
    "\n",
    "directly describe the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:40:26.925571Z",
     "iopub.status.busy": "2024-11-12T14:40:26.924663Z",
     "iopub.status.idle": "2024-11-12T14:40:26.931167Z",
     "shell.execute_reply": "2024-11-12T14:40:26.930099Z",
     "shell.execute_reply.started": "2024-11-12T14:40:26.925521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_zs = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config= genai.GenerationConfig(\n",
    "        temperature=1.0,\n",
    "        top_p=1, #all tokens can become candidates\n",
    "        max_output_tokens=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:45:57.056409Z",
     "iopub.status.busy": "2024-11-12T14:45:57.056055Z",
     "iopub.status.idle": "2024-11-12T14:45:57.436905Z",
     "shell.execute_reply": "2024-11-12T14:45:57.435842Z",
     "shell.execute_reply.started": "2024-11-12T14:45:57.056376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sentiment: **POSITIVE**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "\n",
    "Markdown(model_zs.generate_content(zs_prompt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enum Mode\n",
    "\n",
    "used to constrain the output of prompt to a fixed set of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:45:59.735014Z",
     "iopub.status.busy": "2024-11-12T14:45:59.733954Z",
     "iopub.status.idle": "2024-11-12T14:45:59.739659Z",
     "shell.execute_reply": "2024-11-12T14:45:59.738409Z",
     "shell.execute_reply.started": "2024-11-12T14:45:59.734958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import enum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:48:53.473437Z",
     "iopub.status.busy": "2024-11-12T14:48:53.473026Z",
     "iopub.status.idle": "2024-11-12T14:48:53.845524Z",
     "shell.execute_reply": "2024-11-12T14:48:53.844543Z",
     "shell.execute_reply.started": "2024-11-12T14:48:53.473396Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "model_enum = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ))\n",
    "\n",
    "response = model_enum.generate_content(zs_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot and Few-shot\n",
    "\n",
    "1. Provide example\n",
    "2. Multiple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:49:44.312925Z",
     "iopub.status.busy": "2024-11-12T14:49:44.312449Z",
     "iopub.status.idle": "2024-11-12T14:49:44.318885Z",
     "shell.execute_reply": "2024-11-12T14:49:44.317747Z",
     "shell.execute_reply.started": "2024-11-12T14:49:44.312886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_eg = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:50:54.365505Z",
     "iopub.status.busy": "2024-11-12T14:50:54.364428Z",
     "iopub.status.idle": "2024-11-12T14:50:54.903050Z",
     "shell.execute_reply": "2024-11-12T14:50:54.901810Z",
     "shell.execute_reply.started": "2024-11-12T14:50:54.365427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "\"size\": \"large\",\n",
       "\"type\": \"normal\",\n",
       "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
       "}\n",
       "``` \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "customer_order = \"Give me a large with cheese and pineapple\"\n",
    "\n",
    "Markdown(model_eg.generate_content([few_shot_prompt,customer_order]).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:53:00.538184Z",
     "iopub.status.busy": "2024-11-12T14:53:00.537699Z",
     "iopub.status.idle": "2024-11-12T14:53:01.108271Z",
     "shell.execute_reply": "2024-11-12T14:53:01.107244Z",
     "shell.execute_reply.started": "2024-11-12T14:53:00.538141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class pizza_order(typing.TypedDict):\n",
    "    size : str\n",
    "    ingredients: list[str]\n",
    "    type:str\n",
    "\n",
    "\n",
    "model_pz = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=pizza_order,\n",
    "    ))\n",
    "\n",
    "print(model_pz.generate_content(\"Can I have a large dessert pizza with apple and chocolate\").text)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
